#!rnn.py

batch_size = 10000
batching = 'random'
cache_size = '0'
chunking = '64:32'
combine_error_values = True
dev = { 'class': 'ExternSprintDataset',
  'partitionEpoch': 1,
  'sprintConfigStr': '--config=sprint.dev.config --*.LOGFILE=nn-trainer.dev.log --*.TASK=1 --*.corpus.segment-order-shuffle=true',
  'sprintTrainerExecPath': '/u/raissi/dev/rasr-dense/arch/linux-x86_64-standard/nn-trainer.linux-x86_64-standard'}
device = 'gpu'
gradient_clip = 0
gradient_noise = 0.1
learning_rate = 0.001
learning_rate_control = 'newbob_multi_epoch'
learning_rate_control_min_num_epochs_per_new_lr = 3
learning_rate_control_relative_error_relative_lr = True
learning_rate_file = 'learning_rates'
log = ['./crnn.log']
log_verbosity = 5
max_seqs = 128
min_learning_rate = 2e-05
model = '/work/asr4/raissi/ms-thesis-setups/lm-sa-swb/2021-08--specaugment/work/crnn/sprint_training/CRNNDevSprintTrainingJob.r5oBmZj6aCe7/output/models/epoch'
multiprocessing = True
nadam = True
network = { 'bwd_1': {'L2': 0.01, 'class': 'rec', 'direction': -1, 'dropout': 0.1, 'from': 'source', 'n_out': 512, 'unit': 'nativelstm2'},
  'bwd_2': {'L2': 0.01, 'class': 'rec', 'direction': -1, 'dropout': 0.1, 'from': ['fwd_1', 'bwd_1'], 'n_out': 512, 'unit': 'nativelstm2'},
  'bwd_3': {'L2': 0.01, 'class': 'rec', 'direction': -1, 'dropout': 0.1, 'from': ['fwd_2', 'bwd_2'], 'n_out': 512, 'unit': 'nativelstm2'},
  'bwd_4': {'L2': 0.01, 'class': 'rec', 'direction': -1, 'dropout': 0.1, 'from': ['fwd_3', 'bwd_3'], 'n_out': 512, 'unit': 'nativelstm2'},
  'bwd_5': {'L2': 0.01, 'class': 'rec', 'direction': -1, 'dropout': 0.1, 'from': ['fwd_4', 'bwd_4'], 'n_out': 512, 'unit': 'nativelstm2'},
  'bwd_6': {'L2': 0.01, 'class': 'rec', 'direction': -1, 'dropout': 0.1, 'from': ['fwd_5', 'bwd_5'], 'n_out': 512, 'unit': 'nativelstm2'},
  'center-output': { 'class': 'softmax',
                     'from': 'linear2-diphone',
                     'loss': 'ce',
                     'loss_opts': {'focal_loss_factor': 2.0, 'label_smoothing': 0.2},
                     'target': 'centerState'},
  'centerPhoneme': { 'class': 'eval',
                     'eval': 'tf.floordiv(source(0),47)',
                     'from': 'popFutureLabel',
                     'out_type': {'dim': 47, 'dtype': 'int32', 'sparse': True}},
  'centerState': { 'class': 'eval',
                   'eval': '(source(0)*3)+source(1)',
                   'from': ['centerPhoneme', 'stateId'],
                   'out_type': {'dim': 141, 'dtype': 'int32', 'sparse': True},
                   'register_as_extern_data': 'centerState'},
  'encoder-output': {'class': 'copy', 'from': ['fwd_6', 'bwd_6']},
  'futureLabel': { 'class': 'eval',
                   'eval': 'tf.floormod(source(0),47)',
                   'from': 'popStateId',
                   'out_type': {'dim': 47, 'dtype': 'int32', 'sparse': True},
                   'register_as_extern_data': 'futureLabel'},
  'fwd_1': {'L2': 0.01, 'class': 'rec', 'direction': 1, 'dropout': 0.1, 'from': 'source', 'n_out': 512, 'unit': 'nativelstm2'},
  'fwd_2': {'L2': 0.01, 'class': 'rec', 'direction': 1, 'dropout': 0.1, 'from': ['fwd_1', 'bwd_1'], 'n_out': 512, 'unit': 'nativelstm2'},
  'fwd_3': {'L2': 0.01, 'class': 'rec', 'direction': 1, 'dropout': 0.1, 'from': ['fwd_2', 'bwd_2'], 'n_out': 512, 'unit': 'nativelstm2'},
  'fwd_4': {'L2': 0.01, 'class': 'rec', 'direction': 1, 'dropout': 0.1, 'from': ['fwd_3', 'bwd_3'], 'n_out': 512, 'unit': 'nativelstm2'},
  'fwd_5': {'L2': 0.01, 'class': 'rec', 'direction': 1, 'dropout': 0.1, 'from': ['fwd_4', 'bwd_4'], 'n_out': 512, 'unit': 'nativelstm2'},
  'fwd_6': {'L2': 0.01, 'class': 'rec', 'direction': 1, 'dropout': 0.1, 'from': ['fwd_5', 'bwd_5'], 'n_out': 512, 'unit': 'nativelstm2'},
  'left-output': { 'class': 'softmax',
                   'from': 'linear2-leftContext',
                   'loss': 'ce',
                   'loss_opts': {'focal_loss_factor': 2.0, 'label_smoothing': 0.2},
                   'target': 'pastLabel'},
  'linear1-diphone': {'activation': 'relu', 'class': 'linear', 'from': 'encoder-output', 'n_out': 1088},
  'linear1-leftContext': {'activation': 'relu', 'class': 'linear', 'from': 'encoder-output', 'n_out': 1024},
  'linear1-triphone': {'activation': 'relu', 'class': 'linear', 'from': 'encoder-output', 'n_out': 1344},
  'linear2-diphone': {'activation': 'relu', 'class': 'linear', 'from': 'linear1-diphone', 'n_out': 1088},
  'linear2-leftContext': {'activation': 'relu', 'class': 'linear', 'from': 'linear1-leftContext', 'n_out': 1024},
  'linear2-triphone': {'activation': 'relu', 'class': 'linear', 'from': 'linear1-triphone', 'n_out': 1344},
  'pastLabel': { 'class': 'eval',
                 'eval': 'tf.floormod(source(0),47)',
                 'from': 'popFutureLabel',
                 'out_type': {'dim': 47, 'dtype': 'int32', 'sparse': True},
                 'register_as_extern_data': 'pastLabel'},
  'popFutureLabel': { 'class': 'eval',
                      'eval': 'tf.floordiv(source(0),47)',
                      'from': 'popStateId',
                      'out_type': {'dim': 2209, 'dtype': 'int32', 'sparse': True}},
  'popStateId': { 'class': 'eval',
                  'eval': 'tf.floordiv(source(0),3)',
                  'from': 'data:classes',
                  'out_type': {'dim': 103823, 'dtype': 'int32', 'sparse': True}},
  'right-output': { 'class': 'softmax',
                    'from': 'linear2-triphone',
                    'loss': 'ce',
                    'loss_opts': {'focal_loss_factor': 2.0, 'label_smoothing': 0.2},
                    'target': 'futureLabel'},
  'source': {'class': 'eval', 'eval': "self.network.get_config().typed_value('transform')(source(0, as_data=True), network=self.network)"},
  'stateId': {'class': 'eval', 'eval': 'tf.floormod(source(0),3)', 'from': 'data:classes', 'out_type': {'dim': 3, 'dtype': 'int32', 'sparse': True}}}
newbob_learning_rate_decay = 0.9
newbob_multi_num_epochs = 6
newbob_multi_update_interval = 1
num_epochs = 180
num_outputs = {'classes': [311469, 1], 'data': [240, 2]}
optimizer_epsilon = 1e-08
save_interval = 1
task = 'train'
train = { 'class': 'ExternSprintDataset',
  'partitionEpoch': 6,
  'sprintConfigStr': '--config=sprint.train.config --*.LOGFILE=nn-trainer.train.log --*.TASK=1 --*.corpus.segment-order-shuffle=true',
  'sprintTrainerExecPath': '/u/raissi/dev/rasr-dense/arch/linux-x86_64-standard/nn-trainer.linux-x86_64-standard'}
truncation = -1
use_tensorflow = True
config = {}

locals().update(**config)


import sys
sys.path.append("/u/beck/dev/cachemanager/")
import cachemanager as cm
cm_config = cm.ClientConfiguration()
cm_config.loadDefault()
def cache_file(path):
  client = cm.CmClient(cm_config)
  try:
    return client.fetch(path)[0]
  except Exception:
    return path

from TFUtil import DimensionTag
time_tag = DimensionTag(kind=DimensionTag.Types.Spatial, description="time")

extern_data = {'data': {"dim": 240, "same_dim_tags_as": {"t": time_tag}}}
numbers = [311469, 141, 47, 47, 103823, 47, 2209]
for i, k in enumerate(['classes', 'centerState', 'lastLabel', 'futureLabel', 'popStateId', 'popPastLabel', 'popFutureLabel']):
  extern_data[k] = {'dim': numbers[i], 'dtype': 'int32', 'sparse': True, "same_dim_tags_as": {"t": time_tag},  "available_for_inference": True}
 
# for debug only
def summary(name, x):

  import tensorflow as tf
  # tf.summary.image wants [batch_size, height,  width, channels],
  # we have (batch, time, feature).
  img = tf.expand_dims(x, axis=3)  # (batch,time,feature,1)
  img = tf.transpose(img, [0, 2, 1, 3])  # (batch,feature,time,1)
  tf.summary.image(name, img, max_outputs=10)
  tf.summary.scalar("%s_max_abs" % name, tf.reduce_max(tf.abs(x)))
  mean = tf.reduce_mean(x)
  tf.summary.scalar("%s_mean" % name, mean)
  stddev = tf.sqrt(tf.reduce_mean(tf.square(x - mean)))
  tf.summary.scalar("%s_stddev" % name, stddev)
  tf.summary.histogram("%s_hist" % name, tf.reduce_max(tf.abs(x), axis=2))


def _mask(x, batch_axis, axis, pos, max_amount):
  from returnn.tf.compat import v1 as tf
  ndim = x.get_shape().ndims
  n_batch = tf.shape(x)[batch_axis]
  dim = tf.shape(x)[axis]
  amount = tf.random_uniform(shape=(n_batch,), minval=1, maxval=max_amount + 1, dtype=tf.int32)
  pos2 = tf.minimum(pos + amount, dim)
  idxs = tf.expand_dims(tf.range(0, dim), 0)  # (1,dim)
  pos_bc = tf.expand_dims(pos, 1)  # (batch,1)
  pos2_bc = tf.expand_dims(pos2, 1)  # (batch,1)
  cond = tf.logical_and(tf.greater_equal(idxs, pos_bc), tf.less(idxs, pos2_bc))  # (batch,dim)
  if batch_axis > axis:
    cond = tf.transpose(cond)  # (dim,batch)
  cond = tf.reshape(cond, [tf.shape(x)[i] if i in (batch_axis, axis) else 1 for i in range(ndim)])
  from TFUtil import where_bc
  x = where_bc(cond, 0.0, x)
  return x


def random_mask(x, batch_axis, axis, min_num, max_num, max_dims):

  from returnn.tf.compat import v1 as tf
  n_batch = tf.shape(x)[batch_axis]
  if isinstance(min_num, int) and isinstance(max_num, int) and min_num == max_num:
    num = min_num
  else:
    num = tf.random_uniform(shape=(n_batch,), minval=min_num, maxval=max_num + 1, dtype=tf.int32)
  # https://github.com/tensorflow/tensorflow/issues/9260
  # https://timvieira.github.io/blog/post/2014/08/01/gumbel-max-trick-and-weighted-reservoir-sampling/
  z = -tf.log(-tf.log(tf.random_uniform((n_batch, tf.shape(x)[axis]), 0, 1)))
  _, indices = tf.nn.top_k(z, num if isinstance(num, int) else tf.reduce_max(num))
  # indices should be sorted, and of shape (batch,num), entries (int32) in [0,dim)
  # indices = tf.Print(indices, ["indices", indices, tf.shape(indices)])
  if isinstance(num, int):
    for i in range(num):
      x = _mask(x, batch_axis=batch_axis, axis=axis, pos=indices[:, i], max_amount=max_dims)
  else:
    _, x = tf.while_loop( cond=lambda i, _: tf.less(i, tf.reduce_max(num)),
                          body=lambda i, x: ( i + 1, tf.where( tf.less(i, num),
                          _mask(x, batch_axis=batch_axis, axis=axis, pos=indices[:, i], max_amount=max_dims), x )),
                          loop_vars=(0, x)
                        )
  return x


def transform(data, network):
  # to be adjusted (20-50%)
  max_time_num = 2
  max_time = 15

  max_feature_num = 24
  max_feature = 5

  # halved before this step
  conservatvie_step = 2000

  x = data.placeholder
  from returnn.tf.compat import v1 as tf
  # summary("features", x)
  step = network.global_train_step
  increase_flag = tf.where(tf.greater_equal(step, conservatvie_step), 0, 1)

  def get_masked():
    x_masked = x
    x_masked = random_mask( x_masked, batch_axis=data.batch_dim_axis, axis=data.time_dim_axis,
                            min_num=0, max_num=tf.maximum(tf.shape(x)[data.time_dim_axis]//int(1/0.70*max_time), max_time_num) // (1+increase_flag),
                            max_dims=max_time
                          )
    x_masked = random_mask( x_masked, batch_axis=data.batch_dim_axis, axis=data.feature_dim_axis,
                            min_num=0, max_num=max_feature_num // (1+increase_flag),
                            max_dims=max_feature
                          )
    #summary("features_mask", x_masked)
    return x_masked
  x = network.cond_on_train(get_masked, lambda: x)
  return x

