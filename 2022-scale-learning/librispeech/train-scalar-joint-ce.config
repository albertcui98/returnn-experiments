#!rnn.py



accum_grad_multiple_step = 2
adam = True
batch_size = 10000
batching = 'random'
cache_size = '0'
cleanup_old_models = { 'ignore_score_keys': ['dev_error_ctc', 'dev_error_decision', 'dev_score_ctc', 'train_error_ctc', 'train_error_decision', 'train_score_ctc'],
  'keep': [25, 50, 75, 100],
  'keep_best_n': 2,
  'keep_last_n': 2}
debug_print_layer_output_template = True
dev = { 'audio': { 'norm_mean': '/u/zeyer/setups/librispeech/2018-02-26--att/base/dataset/stats.mean.txt',
             'norm_std_dev': '/u/zeyer/setups/librispeech/2018-02-26--att/base/dataset/stats.std_dev.txt'},
  'bpe': { 'bpe_file': '/u/zeyer/setups/librispeech/2018-02-26--att/base/dataset/trans.bpe.codes',
           'seq_postfix': [0],
           'unknown_label': '<unk>',
           'vocab_file': '/u/zeyer/setups/librispeech/2018-02-26--att/base/dataset/trans.bpe.vocab'},
  'class': 'LibriSpeechCorpus',
  'fixed_random_seed': 1,
  'fixed_random_subset': 3000,
  'path': '/u/zeyer/setups/librispeech/2018-02-26--att/base/dataset/ogg-zips',
  'prefix': 'dev',
  'seq_ordering': 'sorted_reverse',
  'use_cache_manager': True,
  'use_ogg': True,
  'use_zip': True}
device = 'gpu'
gradient_clip = 0
gradient_noise = 0.0
learning_rate = 5.168865511381343e-05
learning_rate_control = 'newbob_multi_epoch'
learning_rate_control_min_num_epochs_per_new_lr = 3
learning_rate_control_relative_error_relative_lr = True
learning_rate_file = 'learning_rates'
log = ['./crnn.log']
log_batch_size = True
log_verbosity = 5
max_seq_length = {'classes': 75}
max_seqs = 200
min_learning_rate = 1.0337731022762685e-06
model = '/u/meyer/setups/work/crnn/training/CRNNTrainingJob.Zr9Mjs8c0duZ/output/models/epoch'
multiprocessing = True
network = { 'conv0': {'activation': None, 'class': 'conv', 'filter_size': (3, 3), 'from': 'source0', 'n_out': 32, 'padding': 'same', 'with_bias': True},
  'conv0p': {'class': 'pool', 'from': 'conv0', 'mode': 'max', 'padding': 'same', 'pool_size': (1, 2)},
  'conv1': {'activation': None, 'class': 'conv', 'filter_size': (3, 3), 'from': 'conv0p', 'n_out': 32, 'padding': 'same', 'with_bias': True},
  'conv1p': {'class': 'pool', 'from': 'conv1', 'mode': 'max', 'padding': 'same', 'pool_size': (1, 2)},
  'conv_merged': {'axes': 'static', 'class': 'merge_dims', 'from': 'conv1p'},
  'ctc': { 'class': 'softmax',
           'from': ['encoder'],
           'loss': 'ctc',
           'loss_opts': {'beam_width': 1, 'ctc_opts': {'ignore_longer_outputs_than_inputs': True}},
           'target': 'classes'},
  'decision': {'class': 'decide', 'from': ['output'], 'loss': 'edit_distance', 'loss_opts': {}, 'target': 'classes'},
  'enc_ctx': {'activation': None, 'class': 'linear', 'from': ['encoder'], 'n_out': 1024, 'with_bias': True},
  'enc_value': {'axis': 'F', 'class': 'split_dims', 'dims': (1, 2048), 'from': ['encoder']},
  'encoder': {'class': 'copy', 'from': ['lstm5_fw', 'lstm5_bw']},
  'inv_fertility': {'activation': 'sigmoid', 'class': 'linear', 'from': ['encoder'], 'n_out': 1, 'with_bias': False},
  'lstm0_bw': {'class': 'rec', 'direction': -1, 'from': ['conv_merged'], 'n_out': 1024, 'unit': 'nativelstm2'},
  'lstm0_fw': {'class': 'rec', 'direction': 1, 'from': ['conv_merged'], 'n_out': 1024, 'unit': 'nativelstm2'},
  'lstm0_pool': {'class': 'pool', 'from': ['lstm0_fw', 'lstm0_bw'], 'mode': 'max', 'padding': 'same', 'pool_size': (3,), 'trainable': False},
  'lstm1_bw': {'class': 'rec', 'direction': -1, 'dropout': 0.3, 'from': ['lstm0_pool'], 'n_out': 1024, 'unit': 'nativelstm2'},
  'lstm1_fw': {'class': 'rec', 'direction': 1, 'dropout': 0.3, 'from': ['lstm0_pool'], 'n_out': 1024, 'unit': 'nativelstm2'},
  'lstm1_pool': {'class': 'pool', 'from': ['lstm1_fw', 'lstm1_bw'], 'mode': 'max', 'padding': 'same', 'pool_size': (2,), 'trainable': False},
  'lstm2_bw': {'class': 'rec', 'direction': -1, 'dropout': 0.3, 'from': ['lstm1_pool'], 'n_out': 1024, 'unit': 'nativelstm2'},
  'lstm2_fw': {'class': 'rec', 'direction': 1, 'dropout': 0.3, 'from': ['lstm1_pool'], 'n_out': 1024, 'unit': 'nativelstm2'},
  'lstm2_pool': {'class': 'pool', 'from': ['lstm2_fw', 'lstm2_bw'], 'mode': 'max', 'padding': 'same', 'pool_size': (1,), 'trainable': False},
  'lstm3_bw': {'class': 'rec', 'direction': -1, 'dropout': 0.3, 'from': ['lstm2_pool'], 'n_out': 1024, 'unit': 'nativelstm2'},
  'lstm3_fw': {'class': 'rec', 'direction': 1, 'dropout': 0.3, 'from': ['lstm2_pool'], 'n_out': 1024, 'unit': 'nativelstm2'},
  'lstm3_pool': {'class': 'pool', 'from': ['lstm3_fw', 'lstm3_bw'], 'mode': 'max', 'padding': 'same', 'pool_size': (1,), 'trainable': False},
  'lstm4_bw': {'class': 'rec', 'direction': -1, 'dropout': 0.3, 'from': ['lstm3_pool'], 'n_out': 1024, 'unit': 'nativelstm2'},
  'lstm4_fw': {'class': 'rec', 'direction': 1, 'dropout': 0.3, 'from': ['lstm3_pool'], 'n_out': 1024, 'unit': 'nativelstm2'},
  'lstm4_pool': {'class': 'pool', 'from': ['lstm4_fw', 'lstm4_bw'], 'mode': 'max', 'padding': 'same', 'pool_size': (1,), 'trainable': False},
  'lstm5_bw': {'class': 'rec', 'direction': -1, 'dropout': 0.3, 'from': ['lstm4_pool'], 'n_out': 1024, 'unit': 'nativelstm2'},
  'lstm5_fw': {'class': 'rec', 'direction': 1, 'dropout': 0.3, 'from': ['lstm4_pool'], 'n_out': 1024, 'unit': 'nativelstm2'},
  'output': { 'cheating': False,
              'class': 'rec',
              'from': [],
              'max_seq_len': "max_len_from('base:encoder')",
              'target': 'classes',
              'unit': { 'accum_att_weights': { 'class': 'eval',
                                               'eval': 'source(0) + source(1) * source(2) * 0.5',
                                               'from': ['prev:accum_att_weights', 'att_weights', 'base:inv_fertility'],
                                               'out_type': {'dim': 1, 'shape': (None, 1)}},
                        'am_scale': { 'add_batch_axis': False,
                                      'class': 'variable',
                                      'init': 'random_normal_initializer(mean=1.0, stddev=0.5)',
                                      'shape': [1]},
                        'att': {'axes': 'except_batch', 'class': 'merge_dims', 'from': ['att0']},
                        'att0': {'base': 'base:enc_value', 'class': 'generic_attention', 'weights': 'att_weights'},
                        'att_weights': {'class': 'softmax_over_spatial', 'from': ['energy']},
                        'combo_output_log_prob': { 'class': 'eval',
                                                   'eval': 'source(2) * safe_log(source(0)) + source(3) * safe_log(source(1)) - '
                                                           'tf.math.reduce_logsumexp(source(2) * safe_log(source(0)) + source(3) * '
                                                           'safe_log(source(1)), axis=-1, keepdims=True)',
                                                   'from': ['output_prob', 'lm_output_prob', 'am_scale', 'lm_scale']},
                        'combo_output_prob': { 'class': 'eval',
                                               'eval': 'tf.exp(source(0))',
                                               'from': ['combo_output_log_prob'],
                                               'loss': 'ce',
                                               'target': 'classes'},
                        'end': {'class': 'compare', 'from': ['output'], 'value': 0},
                        'energy': {'activation': None, 'class': 'linear', 'from': ['energy_tanh'], 'n_out': 1, 'with_bias': False},
                        'energy_in': {'class': 'combine', 'from': ['base:enc_ctx', 'weight_feedback', 's_transformed'], 'kind': 'add', 'n_out': 1024},
                        'energy_tanh': {'activation': 'tanh', 'class': 'activation', 'from': ['energy_in']},
                        'lm_output': { 'class': 'subnetwork',
                                       'from': ['prev:output'],
                                       'load_on_init': '/work/asr4/michel/setups-data/language_modelling/librispeech/neurallm/decoder_sized_transcripts_only/net-model/network.007',
                                       'n_out': 10025,
                                       'subnetwork': { 'input': {'activation': 'identity', 'class': 'linear', 'n_out': 128, 'trainable': False},
                                                       'lstm0': { 'class': 'rnn_cell',
                                                                  'from': ['input'],
                                                                  'n_out': 1000,
                                                                  'trainable': False,
                                                                  'unit': 'LSTMBlock',
                                                                  'unit_opts': {'forget_bias': 0.0}},
                                                       'output': { 'activation': 'identity',
                                                                   'class': 'linear',
                                                                   'from': ['lstm0'],
                                                                   'n_out': 10025,
                                                                   'trainable': False,
                                                                   'use_transposed_weights': False}},
                                       'trainable': False},
                        'lm_output_prob': {'activation': 'softmax', 'class': 'activation', 'from': ['lm_output'], 'target': 'classes'},
                        'lm_scale': { 'add_batch_axis': False,
                                      'class': 'variable',
                                      'init': 'random_normal_initializer(mean=1.0, stddev=0.5)',
                                      'shape': [1]},
                        'output': { 'beam_size': 12,
                                    'cheating': False,
                                    'class': 'choice',
                                    'from': ['combo_output_log_prob'],
                                    'initial_output': 0,
                                    'input_type': 'log_prob',
                                    'target': 'classes'},
                        'output_prob': {'class': 'softmax', 'dropout': 0.3, 'from': ['readout'], 'target': 'classes', 'with_bias': False},
                        'readout': {'class': 'reduce_out', 'from': ['readout_in'], 'mode': 'max', 'num_pieces': 2},
                        'readout_in': {'activation': None, 'class': 'linear', 'from': ['s', 'prev:target_embed', 'att'], 'n_out': 1000},
                        's': {'class': 'rec', 'from': ['prev:target_embed', 'prev:att'], 'n_out': 1000, 'unit': 'nativelstm2'},
                        's_transformed': {'activation': None, 'class': 'linear', 'from': ['s'], 'n_out': 1024, 'with_bias': False},
                        'target_embed': { 'activation': None,
                                          'class': 'linear',
                                          'from': ['output'],
                                          'initial_output': 0,
                                          'n_out': 621,
                                          'with_bias': False},
                        'weight_feedback': { 'activation': None,
                                             'class': 'linear',
                                             'from': ['prev:accum_att_weights'],
                                             'n_out': 1024,
                                             'with_bias': False}}},
  'source': {'class': 'eval', 'eval': "self.network.get_config().typed_value('transform')(source(0), network=self.network)"},
  'source0': {'axis': 'F', 'class': 'split_dims', 'dims': (-1, 1), 'from': 'source'}}
newbob_learning_rate_decay = 0.9
newbob_multi_num_epochs = 20
newbob_multi_update_interval = 1
num_epochs = 100
num_inputs = 40
num_outputs = {'classes': (10025, 1), 'data': (40, 2)}
optimizer_epsilon = 1e-08
preload_from_files = { '0_am_model': { 'filename': '/u/meyer/setups/work/crnn/training/CRNNTrainingJob.zILK2y22CZOm/output/models/epoch.100',
                  'ignore_missing': True,
                  'init_for_train': True,
                  'prefix': ''}}
save_interval = 1
search_output_layer = 'decision'
target = 'classes'
task = 'train'
tf_log_memory_usage = True
train = { 'audio': { 'norm_mean': '/u/zeyer/setups/librispeech/2018-02-26--att/base/dataset/stats.mean.txt',
             'norm_std_dev': '/u/zeyer/setups/librispeech/2018-02-26--att/base/dataset/stats.std_dev.txt'},
  'bpe': { 'bpe_file': '/u/zeyer/setups/librispeech/2018-02-26--att/base/dataset/trans.bpe.codes',
           'seq_postfix': [0],
           'unknown_label': '<unk>',
           'vocab_file': '/u/zeyer/setups/librispeech/2018-02-26--att/base/dataset/trans.bpe.vocab'},
  'class': 'LibriSpeechCorpus',
  'epoch_wise_filter': { (1, 5): {'max_mean_len': 50, 'subdirs': ['train-clean-100', 'train-clean-360'], 'use_new_filter': True},
                         (5, 10): {'max_mean_len': 150, 'subdirs': ['train-clean-100', 'train-clean-360'], 'use_new_filter': True},
                         (11, 20): {'subdirs': ['train-clean-100', 'train-clean-360'], 'use_new_filter': True}},
  'partition_epoch': 20,
  'path': '/u/zeyer/setups/librispeech/2018-02-26--att/base/dataset/ogg-zips',
  'prefix': 'train',
  'seq_ordering': 'laplace:281',
  'use_cache_manager': True,
  'use_ogg': True,
  'use_zip': True}
truncation = -1
update_on_device = True
use_learning_rate_control_always = True
use_tensorflow = True
window = 1
config = {}

locals().update(**config)

def summary(name, x):
    """
    :param str name:
    :param tf.Tensor x: (batch,time,feature)
    """
    import tensorflow as tf
    # tf.summary.image wants [batch_size, height,  width, channels],
    # we have (batch, time, feature).
    img = tf.expand_dims(x, axis=3)  # (batch,time,feature,1)
    img = tf.transpose(img, [0, 2, 1, 3])  # (batch,feature,time,1)
    tf.summary.image(name, img, max_outputs=10)
    tf.summary.scalar("%s_max_abs" % name, tf.reduce_max(tf.abs(x)))
    mean = tf.reduce_mean(x)
    tf.summary.scalar("%s_mean" % name, mean)
    stddev = tf.sqrt(tf.reduce_mean(tf.square(x - mean)))
    tf.summary.scalar("%s_stddev" % name, stddev)
    tf.summary.histogram("%s_hist" % name, tf.reduce_max(tf.abs(x), axis=2))

def mask(x, axis, pos, max_amount):
    """
    :param tf.Tensor x: (batch,time,feature)
    :param int axis:
    :param tf.Tensor pos: (batch,)
    :param int max_amount: inclusive
    """
    import tensorflow as tf
    ndim = x.get_shape().ndims
    n_batch = tf.shape(x)[0]
    dim = tf.shape(x)[axis]
    amount = tf.random_uniform(shape=(n_batch,), minval=1, maxval=max_amount + 1, dtype=tf.int32)
    pos2 = tf.minimum(pos + amount, dim)
    idxs = tf.expand_dims(tf.range(0, dim), 0)  # (1,dim)
    pos_bc = tf.expand_dims(pos, 1)  # (batch,1)
    pos2_bc = tf.expand_dims(pos2, 1)  # (batch,1)
    cond = tf.logical_and(tf.greater_equal(idxs, pos_bc), tf.less(idxs, pos2_bc))  # (batch,dim)
    cond = tf.reshape(cond, [tf.shape(x)[i] if i in (0, axis) else 1 for i in range(ndim)])
    from TFUtil import where_bc
    x = where_bc(cond, 0.0, x)
    return x

def random_mask(x, axis, min_num, max_num, max_dims):
    """
    :param tf.Tensor x: (batch,time,feature)
    :param int axis:
    :param int|tf.Tensor min_num:
    :param int|tf.Tensor max_num: inclusive
    :param int max_dims: inclusive
    """
    import tensorflow as tf
    n_batch = tf.shape(x)[0]
    num = tf.random_uniform(shape=(n_batch,), minval=min_num, maxval=max_num + 1, dtype=tf.int32)
    # https://github.com/tensorflow/tensorflow/issues/9260
    # https://timvieira.github.io/blog/post/2014/08/01/gumbel-max-trick-and-weighted-reservoir-sampling/
    z = -tf.log(-tf.log(tf.random_uniform((n_batch, tf.shape(x)[axis]), 0, 1)))
    _, indices = tf.nn.top_k(z, tf.reduce_max(num))
    # indices should be sorted, and of shape (batch,num), entries (int32) in [0,dim)
    # indices = tf.Print(indices, ["indices", indices, tf.shape(indices)])
    _, x = tf.while_loop(
        cond=lambda i, _: tf.less(i, tf.reduce_max(num)),
        body=lambda i, x: (
            i + 1,
            tf.where(
                tf.less(i, num),
                mask(x, axis=axis, pos=indices[:, i], max_amount=max_dims),
                x)),
        loop_vars=(0, x))
    return x

def transform(x, network):
    import tensorflow as tf
    # summary("features", x)
    x = tf.clip_by_value(x, -3.0, 3.0)
    # summary("features_clip", x)

    def get_masked():
        x_masked = x
        x_masked = random_mask(x_masked, axis=1, min_num=1, max_num=tf.maximum(tf.shape(x)[1] // 100, 1), max_dims=20)
        x_masked = random_mask(x_masked, axis=2, min_num=1, max_num=2, max_dims=num_inputs // 5)
        # summary("features_mask", x_masked)
        return x_masked
    x = network.cond_on_train(get_masked, lambda: x)
    return x

